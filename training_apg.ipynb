{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.8\" # 0.9 causes too much lag. \n",
    "from datetime import datetime\n",
    "import functools\n",
    "\n",
    "# Math\n",
    "import jax.numpy as jp\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import config # Analytical gradients work much better with double precision.\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update('jax_default_matmul_precision', jax.lax.Precision.HIGH)\n",
    "from brax import math\n",
    "\n",
    "# Sim\n",
    "import mujoco\n",
    "import mujoco.mjx as mjx\n",
    "\n",
    "# Brax\n",
    "from brax import envs\n",
    "from brax.base import Motion, Transform\n",
    "from brax.io import mjcf\n",
    "from brax.envs.base import PipelineEnv, State\n",
    "from brax.mjx.pipeline import _reformat_contact\n",
    "from brax.training.acme import running_statistics\n",
    "from brax.io import model\n",
    "\n",
    "# Algorithms\n",
    "from brax.training.agents.apg import train as apg\n",
    "from brax.training.agents.apg import networks as apg_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "\n",
    "# Supporting\n",
    "from etils import epath\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "from ml_collections import config_dict\n",
    "from typing import Any, Dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import distutils.util\n",
    "# import os\n",
    "# xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "# xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "# os.environ['XLA_FLAGS'] = xla_flags\n",
    "# print('Setting environment variable to use GPU rendering:')\n",
    "# %env MUJOCO_GL=egl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_path = epath.Path('anybotics_anymal_c/scene_mjx.xml').as_posix()\n",
    "\n",
    "mj_model = mujoco.MjModel.from_xml_path(xml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cos_wave(t, step_period, scale):\n",
    "    _cos_wave = -jp.cos(((2*jp.pi)/step_period)*t)\n",
    "    return _cos_wave * (scale/2) + (scale/2)\n",
    "\n",
    "def dcos_wave(t, step_period, scale):\n",
    "    \"\"\" \n",
    "    Derivative of the cos wave, for reference velocity\n",
    "    \"\"\"\n",
    "    return ((scale*jp.pi) / step_period) * jp.sin(((2*jp.pi)/step_period)*t)\n",
    "\n",
    "def make_kinematic_ref(sinusoid, step_k, scale=0.3, dt=1/50):\n",
    "    \"\"\" \n",
    "    Makes trotting kinematics for the 12 leg joints.\n",
    "    step_k is the number of timesteps it takes to raise and lower a given foot.\n",
    "    A gait cycle is 2 * step_k * dt seconds long.\n",
    "    \"\"\"\n",
    "    \n",
    "    _steps = jp.arange(step_k)\n",
    "    step_period = step_k * dt\n",
    "    t = _steps * dt\n",
    "    \n",
    "    wave = sinusoid(t, step_period, scale)\n",
    "    # Commands for one step of an active front leg\n",
    "    fleg_cmd_block = jp.concatenate(\n",
    "        [jp.zeros((step_k, 1)),\n",
    "        wave.reshape(step_k, 1),\n",
    "        -2*wave.reshape(step_k, 1)],\n",
    "        axis=1\n",
    "    )\n",
    "    # Our standing config reverses front and hind legs\n",
    "    h_leg_cmd_bloc = -1 * fleg_cmd_block\n",
    "\n",
    "    block1 = jp.concatenate([\n",
    "        jp.zeros((step_k, 3)),\n",
    "        fleg_cmd_block,\n",
    "        h_leg_cmd_bloc,\n",
    "        jp.zeros((step_k, 3))],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    block2 = jp.concatenate([\n",
    "        fleg_cmd_block,\n",
    "        jp.zeros((step_k, 3)),\n",
    "        jp.zeros((step_k, 3)),\n",
    "        h_leg_cmd_bloc],\n",
    "        axis=1\n",
    "    )\n",
    "    # In one step cycle, both pairs of active legs have inactive and active phases\n",
    "    step_cycle = jp.concatenate([block1, block2], axis=0)\n",
    "    return step_cycle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "  def get_default_rewards_config():\n",
    "    default_config = config_dict.ConfigDict(\n",
    "        dict(\n",
    "            scales=config_dict.ConfigDict(\n",
    "              dict(\n",
    "                min_reference_tracking = -2.5 * 3e-3, # to equalize the magnitude\n",
    "                reference_tracking = -1.0,\n",
    "                feet_height = -1.0\n",
    "                )\n",
    "              )\n",
    "            )\n",
    "    )\n",
    "    return default_config\n",
    "\n",
    "  default_config = config_dict.ConfigDict(\n",
    "      dict(rewards=get_default_rewards_config(),))\n",
    "\n",
    "  return default_config\n",
    "\n",
    "# Math functions from (https://github.com/jiawei-ren/diffmimic)\n",
    "def quaternion_to_matrix(quaternions):\n",
    "    r, i, j, k = quaternions[..., 0], quaternions[..., 1], quaternions[..., 2], quaternions[..., 3]\n",
    "    two_s = 2.0 / (quaternions * quaternions).sum(-1)\n",
    "\n",
    "    o = jp.stack(\n",
    "        (\n",
    "            1 - two_s * (j * j + k * k),\n",
    "            two_s * (i * j - k * r),\n",
    "            two_s * (i * k + j * r),\n",
    "            two_s * (i * j + k * r),\n",
    "            1 - two_s * (i * i + k * k),\n",
    "            two_s * (j * k - i * r),\n",
    "            two_s * (i * k - j * r),\n",
    "            two_s * (j * k + i * r),\n",
    "            1 - two_s * (i * i + j * j),\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    return o.reshape(quaternions.shape[:-1] + (3, 3))\n",
    "\n",
    "def matrix_to_rotation_6d(matrix):\n",
    "    batch_dim = matrix.shape[:-2]\n",
    "    return matrix[..., :2, :].reshape(batch_dim + (6,))\n",
    "\n",
    "def quaternion_to_rotation_6d(quaternion):\n",
    "    return matrix_to_rotation_6d(quaternion_to_matrix(quaternion))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrotAnymal(PipelineEnv):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      termination_height: float=0.25,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    step_k = kwargs.pop('step_k', 25)\n",
    "\n",
    "    physics_steps_per_control_step = 10\n",
    "    kwargs['n_frames'] = kwargs.get(\n",
    "        'n_frames', physics_steps_per_control_step)\n",
    "\n",
    "    mj_model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "    kp = 230\n",
    "    mj_model.actuator_gainprm[:, 0] = kp\n",
    "    mj_model.actuator_biasprm[:, 1] = -kp\n",
    "\n",
    "    sys = mjcf.load_model(mj_model)\n",
    "\n",
    "    super().__init__(sys=sys, **kwargs)    \n",
    "    \n",
    "    self.termination_height = termination_height\n",
    "    \n",
    "    self._init_q = mj_model.keyframe('standing').qpos\n",
    "    \n",
    "    self.err_threshold = 0.4 # diffmimic; value from paper.\n",
    "    \n",
    "    self._default_ap_pose = mj_model.keyframe('standing').qpos[7:]\n",
    "    self.reward_config = get_config()\n",
    "\n",
    "    self.action_loc = self._default_ap_pose\n",
    "    self.action_scale = jp.array([0.2, 0.8, 0.8] * 4)\n",
    "    \n",
    "    self.feet_inds = jp.array([21,28,35,42]) # LF, RF, LH, RH\n",
    "\n",
    "    #### Imitation reference\n",
    "    kinematic_ref_qpos = make_kinematic_ref(\n",
    "      cos_wave, step_k, scale=0.3, dt=self.dt)\n",
    "    kinematic_ref_qvel = make_kinematic_ref(\n",
    "      dcos_wave, step_k, scale=0.3, dt=self.dt)\n",
    "    \n",
    "    self.l_cycle = jp.array(kinematic_ref_qpos.shape[0])\n",
    "    \n",
    "    # Expand to entire state space.\n",
    "\n",
    "    kinematic_ref_qpos += self._default_ap_pose\n",
    "    ref_qs = np.tile(self._init_q.reshape(1, 19), (self.l_cycle, 1))\n",
    "    ref_qs[:, 7:] = kinematic_ref_qpos\n",
    "    self.kinematic_ref_qpos = jp.array(ref_qs)\n",
    "    \n",
    "    ref_qvels = np.zeros((self.l_cycle, 18))\n",
    "    ref_qvels[:, 6:] = kinematic_ref_qvel\n",
    "    self.kinematic_ref_qvel = jp.array(ref_qvels)\n",
    "\n",
    "    # Can decrease jit time and training wall-clock time significantly.\n",
    "    self.pipeline_step = jax.checkpoint(self.pipeline_step, \n",
    "      policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\n",
    "    \n",
    "  def reset(self, rng: jax.Array) -> State:\n",
    "    # Deterministic init\n",
    "\n",
    "    qpos = jp.array(self._init_q)\n",
    "    qvel = jp.zeros(18)\n",
    "    \n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    # Position onto ground\n",
    "    pen = jp.min(data.contact.dist)\n",
    "    qpos = qpos.at[2].set(qpos[2] - pen)\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    state_info = {\n",
    "        'rng': rng,\n",
    "        'steps': 0.0,\n",
    "        'reward_tuple': {\n",
    "            'reference_tracking': 0.0,\n",
    "            'min_reference_tracking': 0.0,\n",
    "            'feet_height': 0.0\n",
    "        },\n",
    "        'last_action': jp.zeros(12), # from MJX tutorial.\n",
    "        'kinematic_ref': jp.zeros(19),\n",
    "    }\n",
    "\n",
    "    x, xd = data.x, data.xd\n",
    "    obs = self._get_obs(data.qpos, x, xd, state_info)\n",
    "    reward, done = jp.zeros(2)\n",
    "    metrics = {}\n",
    "    for k in state_info['reward_tuple']:\n",
    "      metrics[k] = state_info['reward_tuple'][k]\n",
    "    state = State(data, obs, reward, done, metrics, state_info)\n",
    "    return jax.lax.stop_gradient(state)\n",
    "  \n",
    "  def step(self, state: State, action: jax.Array) -> State:\n",
    "    action = jp.clip(action, -1, 1) # Raw action\n",
    "\n",
    "    action = self.action_loc + (action * self.action_scale)\n",
    "\n",
    "    data = self.pipeline_step(state.pipeline_state, action)\n",
    "    \n",
    "    ref_qpos = self.kinematic_ref_qpos[jp.array(state.info['steps']%self.l_cycle, int)]\n",
    "    ref_qvel = self.kinematic_ref_qvel[jp.array(state.info['steps']%self.l_cycle, int)]\n",
    "    \n",
    "    # Calculate maximal coordinates\n",
    "    ref_data = data.replace(qpos=ref_qpos, qvel=ref_qvel)\n",
    "    ref_data = mjx.forward(self.sys, ref_data)\n",
    "    ref_x, ref_xd = ref_data.x, ref_data.xd\n",
    "\n",
    "    state.info['kinematic_ref'] = ref_qpos\n",
    "\n",
    "    # observation data\n",
    "    x, xd = data.x, data.xd\n",
    "    obs = self._get_obs(data.qpos, x, xd, state.info)\n",
    "\n",
    "    # Terminate if flipped over or fallen down.\n",
    "    done = 0.0\n",
    "    done = jp.where(x.pos[0, 2] < self.termination_height, 1.0, done)\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    done = jp.where(jp.dot(math.rotate(up, x.rot[0]), up) < 0, 1.0, done)\n",
    "\n",
    "    # reward\n",
    "    reward_tuple = {\n",
    "        'reference_tracking': (\n",
    "          self._reward_reference_tracking(x, xd, ref_x, ref_xd)\n",
    "          * self.reward_config.rewards.scales.reference_tracking\n",
    "        ),\n",
    "        'min_reference_tracking': (\n",
    "          self._reward_min_reference_tracking(ref_qpos, ref_qvel, state)\n",
    "          * self.reward_config.rewards.scales.min_reference_tracking\n",
    "        ),\n",
    "        'feet_height': (\n",
    "          self._reward_feet_height(data.geom_xpos[self.feet_inds][:, 2]\n",
    "                                   ,ref_data.geom_xpos[self.feet_inds][:, 2])\n",
    "          * self.reward_config.rewards.scales.feet_height\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    reward = sum(reward_tuple.values())\n",
    "\n",
    "    # state management\n",
    "    state.info['reward_tuple'] = reward_tuple\n",
    "    state.info['last_action'] = action # used for observation. \n",
    "\n",
    "    for k in state.info['reward_tuple'].keys():\n",
    "      state.metrics[k] = state.info['reward_tuple'][k]\n",
    "\n",
    "    state = state.replace(\n",
    "        pipeline_state=data, obs=obs, reward=reward,\n",
    "        done=done)\n",
    "    \n",
    "    #### Reset state to reference if it gets too far\n",
    "    error = (((x.pos - ref_x.pos) ** 2).sum(-1)**0.5).mean()\n",
    "    to_reference = jp.where(error > self.err_threshold, 1.0, 0.0)\n",
    "\n",
    "    to_reference = jp.array(to_reference, dtype=int) # keeps output types same as input. \n",
    "    ref_data = self.mjx_to_brax(ref_data)\n",
    "\n",
    "    data = jax.tree_util.tree_map(lambda x, y: \n",
    "                                  jp.array((1-to_reference)*x + to_reference*y, x.dtype), data, ref_data)\n",
    "    \n",
    "    x, xd = data.x, data.xd # Data may have changed.\n",
    "    obs = self._get_obs(data.qpos, x, xd, state.info)\n",
    "    \n",
    "    return state.replace(pipeline_state=data, obs=obs)\n",
    "    \n",
    "  def _get_obs(self, qpos: jax.Array, x: Transform, xd: Motion,\n",
    "               state_info: Dict[str, Any]) -> jax.Array:\n",
    "\n",
    "    inv_base_orientation = math.quat_inv(x.rot[0])\n",
    "    local_rpyrate = math.rotate(xd.ang[0], inv_base_orientation)\n",
    "\n",
    "    obs_list = []\n",
    "    # yaw rate\n",
    "    obs_list.append(jp.array([local_rpyrate[2]]) * 0.25)\n",
    "    # projected gravity\n",
    "    obs_list.append(\n",
    "        math.rotate(jp.array([0.0, 0.0, -1.0]), inv_base_orientation))\n",
    "    # motor angles\n",
    "    angles = qpos[7:19]\n",
    "    obs_list.append(angles - self._default_ap_pose)\n",
    "    # last action\n",
    "    obs_list.append(state_info['last_action'])\n",
    "    # kinematic reference\n",
    "    kin_ref = self.kinematic_ref_qpos[jp.array(state_info['steps']%self.l_cycle, int)]\n",
    "    obs_list.append(kin_ref[7:]) # First 7 indicies are fixed\n",
    "\n",
    "    obs = jp.clip(jp.concatenate(obs_list), -100.0, 100.0)\n",
    "\n",
    "    return obs\n",
    "  \n",
    "  def mjx_to_brax(self, data):\n",
    "    \"\"\" \n",
    "    Apply the brax wrapper on the core MJX data structure.\n",
    "    \"\"\"\n",
    "    q, qd = data.qpos, data.qvel\n",
    "    x = Transform(pos=data.xpos[1:], rot=data.xquat[1:])\n",
    "    cvel = Motion(vel=data.cvel[1:, 3:], ang=data.cvel[1:, :3])\n",
    "    offset = data.xpos[1:, :] - data.subtree_com[self.sys.body_rootid[1:]]\n",
    "    offset = Transform.create(pos=offset)\n",
    "    xd = offset.vmap().do(cvel)\n",
    "    data = _reformat_contact(self.sys, data)\n",
    "    return data.replace(q=q, qd=qd, x=x, xd=xd)\n",
    "\n",
    "\n",
    "  # ------------ reward functions----------------\n",
    "  def _reward_reference_tracking(self, x, xd, ref_x, ref_xd):\n",
    "    \"\"\"\n",
    "    Rewards based on inertial-frame body positions.\n",
    "    Notably, we use a high-dimension representation of orientation.\n",
    "    \"\"\"\n",
    "\n",
    "    f = lambda x, y: ((x - y) ** 2).sum(-1).mean()\n",
    "\n",
    "    _mse_pos = f(x.pos,  ref_x.pos)\n",
    "    _mse_rot = f(quaternion_to_rotation_6d(x.rot),\n",
    "                 quaternion_to_rotation_6d(ref_x.rot))\n",
    "    _mse_vel = f(xd.vel, ref_xd.vel)\n",
    "    _mse_ang = f(xd.ang, ref_xd.ang)\n",
    "\n",
    "    # Tuned to be about the same size.\n",
    "    return _mse_pos      \\\n",
    "      + 0.1 * _mse_rot   \\\n",
    "      + 0.01 * _mse_vel  \\\n",
    "      + 0.001 * _mse_ang\n",
    "\n",
    "  def _reward_min_reference_tracking(self, ref_qpos, ref_qvel, state):\n",
    "    \"\"\" \n",
    "    Using minimal coordinates. Improves accuracy of joint angle tracking.\n",
    "    \"\"\"\n",
    "    pos = jp.concatenate([\n",
    "      state.pipeline_state.qpos[:3],\n",
    "      state.pipeline_state.qpos[7:]])\n",
    "    pos_targ = jp.concatenate([\n",
    "      ref_qpos[:3],\n",
    "      ref_qpos[7:]])\n",
    "    pos_err = jp.linalg.norm(pos_targ - pos)\n",
    "    vel_err = jp.linalg.norm(state.pipeline_state.qvel- ref_qvel)\n",
    "\n",
    "    return pos_err + vel_err\n",
    "\n",
    "  def _reward_feet_height(self, feet_pos, feet_pos_ref):\n",
    "    return jp.sum(jp.abs(feet_pos - feet_pos_ref)) # try to drive it to 0 using the l1 norm.\n",
    "\n",
    "envs.register_environment('trotting_anymal', TrotAnymal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_networks_factory = functools.partial(\n",
    "    apg_networks.make_apg_networks,\n",
    "    hidden_layer_sizes=(256, 128)\n",
    ")\n",
    "\n",
    "epochs = 499\n",
    "\n",
    "train_fn = functools.partial(apg.train,\n",
    "                             episode_length=240,\n",
    "                             policy_updates=epochs,\n",
    "                             horizon_length=32,\n",
    "                             num_envs=264,\n",
    "                             learning_rate=1e-4,\n",
    "                             num_eval_envs=64,\n",
    "                             num_evals=10 + 1,\n",
    "                             use_float64=True,\n",
    "                             normalize_observations=True,\n",
    "                             network_factory=make_networks_factory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "\n",
    "def progress(it, metrics):\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(it)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "# Each foot contacts the ground twice/sec.\n",
    "env = envs.get_environment(\"trotting_anymal\", step_k = 13)\n",
    "eval_env = envs.get_environment(\"trotting_anymal\", step_k = 13)\n",
    "\n",
    "make_inference_fn, params, _= train_fn(environment=env,\n",
    "                                       progress_fn=progress,\n",
    "                                       eval_env=eval_env)\n",
    "\n",
    "plt.errorbar(x_data, y_data, yerr=ydataerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newBrax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
