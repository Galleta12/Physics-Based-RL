{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Sequence, Tuple, Union\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.envs.base import Env, PipelineEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "# from brax.training.agents.apg import train as apg\n",
    "# from brax.training.agents.apg import networks as apg_networks\n",
    "from brax.io import html, mjcf, model\n",
    "from etils import epath\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from ml_collections import config_dict\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "from jax import vmap\n",
    "import jax.random\n",
    "from jax import lax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment variable to use GPU rendering:\n",
      "env: MUJOCO_GL=egl\n"
     ]
    }
   ],
   "source": [
    "import distutils.util\n",
    "import os\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "print('Setting environment variable to use GPU rendering:')\n",
    "%env MUJOCO_GL=egl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.8\" # 0.9 causes too much lag. \n",
    "from datetime import datetime\n",
    "import functools\n",
    "\n",
    "# Math\n",
    "import jax.numpy as jp\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import config # Analytical gradients work much better with double precision.\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "config.update('jax_default_matmul_precision', jax.lax.Precision.HIGH)\n",
    "from brax import math\n",
    "\n",
    "# Sim\n",
    "import mujoco\n",
    "import mujoco.mjx as mjx\n",
    "\n",
    "# Brax\n",
    "from brax import envs\n",
    "from brax.base import Motion, Transform\n",
    "from brax.io import mjcf\n",
    "from brax.envs.base import PipelineEnv, State\n",
    "from brax.mjx.pipeline import _reformat_contact\n",
    "from brax.training.acme import running_statistics\n",
    "from brax.io import model\n",
    "\n",
    "# Algorithms\n",
    "# from brax.training.agents.apg import train as apg\n",
    "# from brax.training.agents.apg import networks as apg_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "\n",
    "# Supporting\n",
    "from etils import epath\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "from ml_collections import config_dict\n",
    "from typing import Any, Dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SimpleConverter import SimpleConverter\n",
    "from agent_mimic_env.agent_template import HumanoidTemplate\n",
    "from agent_mimic_env.agent_eval_template import HumanoidEvalTemplate\n",
    "from agent_mimic_env.agent_training_template import HumanoidTrainTemplate\n",
    "from agent_mimic_env.agent_test_apg import HumanoidAPGTest\n",
    "from agent_mimic_env.agent_eval_ppo import HumanoidPPOEval\n",
    "from utils.util_data import *\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agent_mimic_env\n",
    "from agent_mimic_env import register_mimic_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from box import Box\n",
    "# Path to your YAML file\n",
    "yaml_file_path = 'config_params/punch.yaml'\n",
    "# Load the YAML file\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    args = Box(yaml.safe_load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_mimic_env.pds_controllers_agents import feedback_pd_controller, stable_pd_controller_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# env_replay,env_eval, env,env_apg,env_eval_ppo=register_mimic_env(args)\n",
    "\n",
    "# #for the eval here we run the trained policy\n",
    "# # jit_reset = jax.jit(env_eval.reset)\n",
    "# # jit_step = jax.jit(env_eval.step)\n",
    "\n",
    "# env_eval_ppo.set_pd_callback(stable_pd_controller_action)\n",
    "\n",
    "# env_eval.set_pd_callback(stable_pd_controller_action)\n",
    "\n",
    "# env.set_pd_callback(stable_pd_controller_action)\n",
    "#env_apg.set_pd_callback(stable_pd_controller_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = SimpleConverter(args.ref)\n",
    "trajectory.load_mocap()\n",
    "model_path = 'models/final_humanoid.xml'\n",
    "envs.register_environment('humanoidPPOEval',HumanoidPPOEval)\n",
    "env_name_ppo_eval = 'humanoidPPOEval'\n",
    "env_kwargs = dict(referece_data=trajectory,model_path=model_path,args=args) \n",
    "    \n",
    "env_eval_ppo = envs.get_environment(env_name_ppo_eval,**env_kwargs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "episode_len = env_eval_ppo.rollout_lenght\n",
    "print(episode_len)\n",
    "#print(env.err_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function make_ppo_networks at 0x7fa40b6dede0>, policy_hidden_layer_sizes=(512, 256), value_hidden_layer_sizes=(512, 256))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "make_networks_factory = functools.partial(\n",
    "    ppo_networks.make_ppo_networks,\n",
    "        policy_hidden_layer_sizes=(512,256),\n",
    "        value_hidden_layer_sizes=(512,256))\n",
    "make_networks_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = functools.partial(\n",
    "    ppo.train, num_timesteps=1000, num_evals=10,\n",
    "    episode_length=episode_len-1, normalize_observations=True, action_repeat=1,\n",
    "    unroll_length=15, num_minibatches=24, num_updates_per_batch=8,\n",
    "    discounting=0.97, learning_rate=3e-4, entropy_cost=1e-3, num_envs=256,\n",
    "    batch_size=256, seed=0,network_factory=make_networks_factory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: traced array with shape int64[].\nThe problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or `jnp.array(x, int)` instead.\nThe error occurred while tracing the function generate_eval_unroll at /home/user/anaconda3/envs/newBrax/lib/python3.11/site-packages/brax/training/acting.py:103 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a:i64[] = convert_element_type[new_dtype=int64 weak_type=False] b\n    from line /home/user/workspace/RLPhysics/Physics-Based-RL/agent_mimic_env/agent_eval_ppo.py:155:15 (HumanoidPPOEval.reset)\n\n  operation a:i64[] = convert_element_type[new_dtype=int64 weak_type=False] b\n    from line /home/user/workspace/RLPhysics/Physics-Based-RL/agent_mimic_env/agent_eval_ppo.py:155:15 (HumanoidPPOEval.reset)\n\n  operation a:i64[] = convert_element_type[new_dtype=int64 weak_type=False] b\n    from line /home/user/workspace/RLPhysics/Physics-Based-RL/agent_mimic_env/agent_eval_ppo.py:155:15 (HumanoidPPOEval.reset)\n\n  operation a:f64[] = convert_element_type[new_dtype=float64 weak_type=False] b\n    from line /home/user/workspace/RLPhysics/Physics-Based-RL/agent_mimic_env/agent_eval_ppo.py:155:15 (HumanoidPPOEval.reset)\n\n  operation a:f64[] = convert_element_type[new_dtype=float64 weak_type=False] b\n    from line /home/user/workspace/RLPhysics/Physics-Based-RL/agent_mimic_env/agent_eval_ppo.py:155:15 (HumanoidPPOEval.reset)\n\n(Additional originating lines are not shown.)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m   plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m     36\u001b[0m   plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 39\u001b[0m make_inference_fn, params, _\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_eval_ppo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mprogress_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_eval_ppo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime to jit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtimes[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime to train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtimes[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/newBrax/lib/python3.11/site-packages/brax/training/agents/ppo/train.py:405\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(environment, num_timesteps, episode_length, action_repeat, num_envs, max_devices_per_host, num_eval_envs, learning_rate, entropy_cost, discounting, seed, unroll_length, batch_size, num_minibatches, num_updates_per_batch, num_evals, num_resets_per_eval, normalize_observations, reward_scaling, clipping_epsilon, gae_lambda, deterministic_eval, network_factory, progress_fn, normalize_advantage, eval_env, policy_params_fn, randomization_fn)\u001b[0m\n\u001b[1;32m    403\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_evals \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 405\u001b[0m   metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_unpmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m          \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalizer_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtraining_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m   logging\u001b[38;5;241m.\u001b[39minfo(metrics)\n\u001b[1;32m    410\u001b[0m   progress_fn(\u001b[38;5;241m0\u001b[39m, metrics)\n",
      "File \u001b[0;32m~/anaconda3/envs/newBrax/lib/python3.11/site-packages/brax/training/acting.py:125\u001b[0m, in \u001b[0;36mEvaluator.run_evaluation\u001b[0;34m(self, policy_params, training_metrics, aggregate_episodes)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key, unroll_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key)\n\u001b[1;32m    124\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 125\u001b[0m eval_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_eval_unroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m eval_metrics \u001b[38;5;241m=\u001b[39m eval_state\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metrics\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    127\u001b[0m eval_metrics\u001b[38;5;241m.\u001b[39mactive_episodes\u001b[38;5;241m.\u001b[39mblock_until_ready()\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/newBrax/lib/python3.11/site-packages/brax/training/acting.py:107\u001b[0m, in \u001b[0;36mEvaluator.__init__.<locals>.generate_eval_unroll\u001b[0;34m(policy_params, key)\u001b[0m\n\u001b[1;32m    105\u001b[0m reset_keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key, num_eval_envs)\n\u001b[1;32m    106\u001b[0m eval_first_state \u001b[38;5;241m=\u001b[39m eval_env\u001b[38;5;241m.\u001b[39mreset(reset_keys)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_unroll\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_first_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_policy_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43munroll_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maction_repeat\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/newBrax/lib/python3.11/site-packages/brax/training/acting.py:75\u001b[0m, in \u001b[0;36mgenerate_unroll\u001b[0;34m(env, env_state, policy, key, unroll_length, extra_fields)\u001b[0m\n\u001b[1;32m     71\u001b[0m   nstate, transition \u001b[38;5;241m=\u001b[39m actor_step(\n\u001b[1;32m     72\u001b[0m       env, state, policy, current_key, extra_fields\u001b[38;5;241m=\u001b[39mextra_fields)\n\u001b[1;32m     73\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (nstate, next_key), transition\n\u001b[0;32m---> 75\u001b[0m (final_state, _), data \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munroll_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_state, data\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/newBrax/lib/python3.11/site-packages/jax/_src/core.py:1481\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m-> 1481\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ConcretizationTypeError(arg, fname_context)\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: traced array with shape int64[].\nThe problem arose with the `int` function. If trying to convert the data type of a value, try using `x.astype(int)` or `jnp.array(x, int)` instead.\nThe error occurred while tracing the function generate_eval_unroll at /home/user/anaconda3/envs/newBrax/lib/python3.11/site-packages/brax/training/acting.py:103 for jit. This value became a tracer due to JAX operations on these lines:\n\n  operation a:i64[] = convert_element_type[new_dtype=int64 weak_type=False] b\n    from line /home/user/workspace/RLPhysics/Physics-Based-RL/agent_mimic_env/agent_eval_ppo.py:155:15 (HumanoidPPOEval.reset)\n\n  operation a:i64[] = convert_element_type[new_dtype=int64 weak_type=False] b\n    from line /home/user/workspace/RLPhysics/Physics-Based-RL/agent_mimic_env/agent_eval_ppo.py:155:15 (HumanoidPPOEval.reset)\n\n  operation a:i64[] = convert_element_type[new_dtype=int64 weak_type=False] b\n    from line /home/user/workspace/RLPhysics/Physics-Based-RL/agent_mimic_env/agent_eval_ppo.py:155:15 (HumanoidPPOEval.reset)\n\n  operation a:f64[] = convert_element_type[new_dtype=float64 weak_type=False] b\n    from line /home/user/workspace/RLPhysics/Physics-Based-RL/agent_mimic_env/agent_eval_ppo.py:155:15 (HumanoidPPOEval.reset)\n\n  operation a:f64[] = convert_element_type[new_dtype=float64 weak_type=False] b\n    from line /home/user/workspace/RLPhysics/Physics-Based-RL/agent_mimic_env/agent_eval_ppo.py:155:15 (HumanoidPPOEval.reset)\n\n(Additional originating lines are not shown.)\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "y_pose_error = []  # to store pose error\n",
    "times = [datetime.now()]\n",
    "max_y_rewards, min_y_rewards = 0, -5\n",
    "def progress(num_steps, metrics):\n",
    "  \n",
    "  #print(num_steps)\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(num_steps)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "  #y_pose_error.append(metrics['eval/episode_pose_error'])  # capture pose error\n",
    "  \n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.subplot(1, 2, 1) \n",
    "  plt.xlim([0, num_steps* 1.25])\n",
    "  plt.ylim([max_y_rewards, min_y_rewards])\n",
    "\n",
    "  plt.xlabel('# environment steps')\n",
    "  plt.ylabel('reward per episode')\n",
    "  plt.title(f'Latest Reward: {y_data[-1]:.3f}')\n",
    "  plt.plot(x_data, y_data, '-o')\n",
    "  \n",
    "  # plt.subplot(1, 2, 2)\n",
    "  # plt.xlim([0, num_steps* 1.25])\n",
    "  # plt.ylim([min(y_pose_error) * 0.9, max(y_pose_error) * 1.1])\n",
    "  # plt.xlabel('# Environment Steps')\n",
    "  # plt.ylabel('Pose Error per Episode')\n",
    "  # plt.title(f'Latest Pose Error: {y_pose_error[-1]:.3f}')\n",
    "  # plt.plot(x_data, y_pose_error, '-o')\n",
    "  \n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  \n",
    "\n",
    "make_inference_fn, params, _= train_fn(environment=env_eval_ppo,\n",
    "                                       progress_fn=progress,\n",
    "                                       eval_env=env_eval_ppo)\n",
    "\n",
    "print(f'time to jit: {times[1] - times[0]}')\n",
    "print(f'time to train: {times[-1] - times[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
